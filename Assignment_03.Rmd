---
title: "Assignment_3"
author: 'Yingtong Zhang'
date: "Jan 31, 2019"
output: html_document
---

The objective of today's exercise is to provide a quick introduction to some common tools for dealing with big data. For each tool we are just using the most basic syntax and you are encouraged to go back and read the help for each at a later date. This exercise also focuses on "general purpose" tools. There are a multitude of R libraries available for accessing specific data sources and web services. A quick summary of some of these is available at http://cran.r-project.org/web/views/WebTechnologies.html. In addition, a Google search on many of the tools and topics covered in Chapters 3 and 4 will provide a lot of additional info on big data tools outside of R.

Note: The code in this exercise will download data off the web dynamically, which can take some time, so try to "knit" infrequently.

```{r,echo=FALSE}
## since libraries will be pulled, make sure repository is set
repos = "http://cran.us.r-project.org"
get.pkg <- function(pkg){
  loaded <- do.call("require",list(package=pkg))
  if(!loaded){
    print(paste("trying to install",pkg))
    install.packages(pkg,dependencies=TRUE,repos=repos)
    loaded <- do.call("require",list(package=pkg))
    if(loaded){
      print(paste(pkg,"installed and loaded"))
    } 
    else {
      stop(paste("could not install",pkg))
    }    
  }
}
get.pkg("RCurl")
get.pkg("XML")
get.pkg("ncdf4")
get.pkg("devtools")
library("MODISTools")
```



**Question 1:**

Using the US Forest Service's Forest Inventory and Analysis (FIA) data set, plot the rank vs log(abundance) curve for tree seedling counts from Rhode Island. Data is available at https://apps.fs.usda.gov/fia/datamart/CSV/RI_SEEDLING.csv and the relevant columns are TREECOUNT (raw seedling counts) and SPCD (species codes). 
Hints: tapply, sum, na.rm=TRUE, sort, decreasing=TRUE, log='y'

```{r}
data_FIA = read.csv("https://apps.fs.usda.gov/fia/datamart/CSV/RI_SEEDLING.csv")
count_by_code <- sort(tapply(data_FIA$TREECOUNT, data_FIA$SPCD, sum, na.rm = TRUE), decreasing = TRUE)

plot(1:length(count_by_code), log(count_by_code), type='l', xlab = 'Abundance Rank', ylab = 'Abundance(log)')
```



**Question 2:**

Create a sorted table of how many FLUXNET eddy-covariance towers are in each country according to the website at http://fluxnet.fluxdata.org/sites/site-list-and-pages/. 
Hint: use substring to extract the country code from the overall FLUXNET ID code.

```{r}
nu <- function(x){as.numeric(as.character(x))}  ## simple function to convert data to numeric

# as the getURL function doesn't work to get the table because of the format change of the webpage, I save the page to html to read as a table. 
fluxnet_table = readHTMLTable("FLUXNET2015Sites.html")[[1]]
country_ID <- table(substr(fluxnet_table$SITE_ID, 1, 2))
towers_num <- as.data.frame(sort(country_ID))
colnames(towers_num) <- c('Country', 'count')
towers_num

```



**Question 3:** 

Within the object myCode, find all the lines that begin with the comment character, #.
```{r}
myCode = readLines("Exercise_03_BigData.Rmd")  ## read unstructured text
x = grep("^#", myCode)    ## returns the line numbers that include the string 'RI'
myCode[x]

```


**Question 4:** 

Similar to how we can point read.csv to the URL of a text file, you can open and manipulate netCDF files on remote servers if those servers support THREDDS/OpenDAP. Furthermore, these utilities let you grab just the part of the file that you need rather than the file in it's entirety. Using this approach, download and plot the air temperature data for Boston for 2004 that's located on the ORNL DAAC server `http://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1220/mstmip_driver_global_hd_climate_tair_2004_v1.nc4`.  The underlying file is quite large so make sure to grab just the subset you need. To do so you'll need to first grab the lat, lon, and time variables to find _which_ grid cell to grab for lat and lon and how many values to grab from time (i.e. _length_). 


```{r}
ORNL = nc_open("http://thredds.daac.ornl.gov/thredds/dodsC/ornldaac/1220/mstmip_driver_global_hd_climate_tair_2004_v1.nc4")
print(ORNL)

time = ncvar_get(ORNL, "time_bnds")
lat = ncvar_get(ORNL, "lat_bnds")
lon = ncvar_get(ORNL, "lon_bnds")

#Boston: lat:42.3601, lon:71.0589
start_lat = which((lat[1,] >= 42) & (lat[1,] <= 42.5))[1]
start_lon = which((lon[1,] >= -71.5) & (lon[1,] <= -71))[1]

air_temp = ncvar_get(ORNL,"tair", start = c(start_lon, start_lat, 1), count = c(1,1,-1))

start_date = as.Date("1700-01-01")
time_julian = start_date + time[1,]
plot(time_julian, air_temp, type = 'l', main = "2014 Boston Air Temperature", xlab = "date", ylab = "Air Temperature (K)")

nc_close(ORNL)

```



**Question 5:** Plot EVI versus time and compare to the CO2 flux observations.

```{r}
# EVI data
MODISTools::mt_products()
MODISTools::mt_bands(product="MOD13Q1")

WC_file = "MODIS.WillowCreek.RData"
if(file.exists(WC_file)){
  load(WC_file)
} else {
  subset <- MODISTools::mt_subset(product = "MOD13Q1",
                                band = "250m_16_days_EVI",
                                lat=46.0827,
                                lon=-89.9792,
                                start="2012-01-01",
                                end="2012-12-31",
                                km_lr = 1,
                                km_ab = 1,
                                site_name = "WillowCreek")
  save(subset,file=WC_file)
}
subset$header
head(subset$data)


# CO2 flux
wlef = nc_open("US-PFa-WLEF-TallTowerClean-2012-L0-vFeb2013.nc")

height = ncvar_get(wlef,"M_lvl")  
FoY = ncvar_get(wlef,"FoY")  
start = which((FoY >= 2012) & (FoY < 2013))[1]
NEE = ncvar_get(wlef,"NEE_co2", start = c(1, start) , count =c(1, -1))    ## NEE data

doy = ncvar_get(wlef,"time")
start_date = as.Date("2011-12-31")
time_julian = start_date + doy

## average EVI spatially & use 'scale' to set units
EVI = tapply(subset$data$data, subset$data$calendar_date, mean,na.rm=TRUE) * as.numeric(subset$header$scale)
time = as.Date(names(EVI))

# plot
par(mar = c(6, 4, 4, 4) + 1.2) 
plot(time, EVI, pch=16, axes=FALSE, ylim=c(0,1), xlab="", ylab="", type="l",col="black")
axis(2, ylim=c(0,1),col="black",las=1)  
mtext("EVI",side=2,line=2.5)
box()

par(new = TRUE)

plot(time_julian, filter(NEE,rep(1/24,24)), pch=15,  xlab="", ylab="", ylim=c(-6,4), axes=FALSE, type="l", col="red")
mtext("CO2 Flux observtions",side=4,col="red",line=4) 
axis(4, c(-6, -4, -2, 0, 2, 4), col="red",col.axis="red",las=1)

## Draw the time axis
axis(1, time, format(time, "%b %d"), cex.axis = .7)
mtext("Date",side=1,col="black",line=2.5)  
## Add Legend
legend("topleft",legend=c("EVI","CO2 Flux"),
  text.col=c("black","red"), lty = c(1,1),col=c("black","red"))

nc_close(wlef)
```



**Question #6:**

Imagine you are working with the full FIA database and want to ensure that the data you are using is always up to date. However, the total size of the database is large, the USFS server is slow, and you don't want to completely delete and reinstall the database every day when only a small percentage of the data changes in any update. 

* Write out the pseudocode/outline for how to keep the files up to date
* Write out what the cron table would look like to schedule this job (assume the update only needs to be done weekly)


```
First, a batch file is needed to excute the grab the data from the FIA database

The batch file could like:
#!/bin/sh
#/usr/ytzhang/update_FIA_monthly.sh: monthly maintenance script
#make directory for the downloading updated files, and make sure the files would not be rewrite
mkdir -p /path/to
usr/bin/wget http://domain.com/file.tar.gz -O /path/to/file_$(date +%Y%m%d_%H%M%S).tar.gz


Sencond, using crontab -e and running the command like: 
MAILTO=zhangyt@bu.edu
update time /usr/ytzhang/update_FIA_monthly.sh


The scheduled job runs once a week at 12:00 am on Sunday and the cron table would be like: 
```
```
0 0 * * 0 /usr/ytzhang/update_FIA_monthly.sh
```






